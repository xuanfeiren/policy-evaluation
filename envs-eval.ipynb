{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PPO = PPO.load(\"ppo_cartpole\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "feature_dim = 100  # Example feature dimension\n",
    "repeat = 1\n",
    "gamma = 0.9\n",
    "num_grids = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00314461,  0.00537847,  0.02909007,  0.00295134], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('MlpPolicy', env, verbose=1)\n",
    "# model.learn(total_timesteps=100000)\n",
    "# model.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "obs\n",
    "action = model_PPO.predict(obs)[0]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_unif(s):\n",
    "  a = env.action_space.sample()\n",
    "  return a\n",
    "\n",
    "def policy_PPO(s):\n",
    "  action = model_PPO.predict(s)[0]\n",
    "  return action\n",
    "def policy_mix(mix):\n",
    "    \"\"\"\n",
    "    Creates a policy function with fixed mix parameter\n",
    "    \n",
    "    Args:\n",
    "        mix: Probability of using PPO policy (default 0.75)\n",
    "    \n",
    "    Returns:\n",
    "        Function that takes only state parameter\n",
    "    \"\"\"\n",
    "    def policy(s):\n",
    "        if np.random.rand() < mix:\n",
    "            return policy_PPO(s)\n",
    "        else:\n",
    "            return policy_unif(s)\n",
    "    return policy\n",
    "def rbf_random_fourier_features(state, action, feature_dim = feature_dim, length_scale=1.0):\n",
    "    np.random.seed(0)\n",
    "    state_array = np.array(state, dtype=np.float32).reshape(-1)\n",
    "    action_array = np.array([float(action)])\n",
    "    state_action = np.concatenate((state_array, action_array))\n",
    "    dim = state_action.shape[0]\n",
    "    \n",
    "    # Handle even/odd feature dimensions\n",
    "    if feature_dim % 2 == 0:\n",
    "        d_cos = d_sin = feature_dim // 2\n",
    "    else:\n",
    "        d_cos = (feature_dim + 1) // 2\n",
    "        d_sin = (feature_dim - 1) // 2\n",
    "    \n",
    "    omega = np.random.normal(scale=1.0/length_scale, size=(dim, d_cos))\n",
    "    bias = np.random.uniform(0, 2 * np.pi, size=d_cos)\n",
    "    z = state_action @ omega + bias\n",
    "    cos_features = np.cos(z)\n",
    "    sin_features = np.sin(z[:d_sin]) if d_sin > 0 else np.array([])\n",
    "    feature = np.sqrt(1.0 / feature_dim) * np.concatenate([cos_features, sin_features])\n",
    "    return feature\n",
    "\n",
    "def collect_trajectory(policy, feature_dim):\n",
    "    s0, _ = env.reset()\n",
    "    traj_list = [s0]\n",
    "    while True:\n",
    "        a0 = policy(s0)\n",
    "        phi_sa = rbf_random_fourier_features(s0, a0, feature_dim)\n",
    "        traj_list.append(phi_sa)\n",
    "        s1, r0,  terminated, truncated, _ = env.step(a0)\n",
    "        traj_list.append(r0)\n",
    "        traj_list.append(s1)\n",
    "        s0 = s1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    # print(len(traj_list))\n",
    "    return traj_list[:-1]  # removing the terminal state\n",
    "\n",
    "def collect_data(n, policy, feature_dim=feature_dim):\n",
    "    data = []\n",
    "    while len(data) < n:\n",
    "        trajectory = collect_trajectory(policy, feature_dim)\n",
    "        i = 0\n",
    "        while i < len(trajectory)-3:\n",
    "            state = trajectory[i]\n",
    "            action = policy(state)\n",
    "            phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "            reward = trajectory[i+2]\n",
    "            next_state = trajectory[i+3]\n",
    "            next_action = policy(next_state)\n",
    "            phi_sa_prime = rbf_random_fourier_features(next_state, next_action, feature_dim)\n",
    "            \n",
    "            data.append((phi_sa, reward, phi_sa_prime))\n",
    "            i += 3\n",
    "            if len(data) >= n:\n",
    "                break\n",
    "\n",
    "    return data[:n]  # Return exactly n samples as a single array\n",
    "\n",
    "def Q(state, action, theta,feature_dim=feature_dim):\n",
    "    phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    return np.dot(theta, phi_sa)\n",
    "\n",
    "def policy_eval_LSTD(theta_init,data, feature_dim=feature_dim, alpha=0.01):\n",
    "    '''Use TD(0) which converges to the solution of LSTD'''\n",
    "    theta_lstd = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        Q_sa = np.dot(theta_lstd, phi_sa)\n",
    "        Q_sa_prime = np.dot(theta_lstd, phi_sa_prime)\n",
    "        td_error = reward + gamma * Q_sa_prime - Q_sa\n",
    "        theta_lstd += alpha * td_error * phi_sa\n",
    "    \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_lstd, phi_sa)\n",
    "    \n",
    "    return theta_lstd\n",
    "\n",
    "def policy_eval_BRM(theta_init, data,  feature_dim=feature_dim, learning_rate=0.1):\n",
    "    theta_BRM = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        x_sa = phi_sa - gamma * phi_sa_prime\n",
    "        gradient = -2 * (reward - np.dot(x_sa, theta_BRM)) * x_sa\n",
    "        theta_BRM -= learning_rate * gradient\n",
    "        \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_BRM, phi_sa)\n",
    "    \n",
    "    return theta_BRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02547322,  0.06252606,  0.09388342, -0.02458602,  0.02528511,\n",
       "       -0.0207858 , -0.03807906, -0.04223548, -0.09502348, -0.00679444,\n",
       "       -0.06593721, -0.0279915 , -0.09681819,  0.07962164, -0.00397882,\n",
       "        0.0705046 , -0.09705082,  0.01719915, -0.05296642,  0.09715305,\n",
       "       -0.06175336,  0.09438656, -0.08940489, -0.09937284, -0.09981516,\n",
       "       -0.01168766, -0.01801703,  0.07728874, -0.07611552,  0.09671857,\n",
       "        0.01785016,  0.03611199, -0.0999945 , -0.08458896,  0.0610783 ,\n",
       "       -0.03909919, -0.06677396, -0.08596892,  0.08404027, -0.01774878,\n",
       "       -0.05181941, -0.02109854, -0.00713961, -0.09444217,  0.01081733,\n",
       "        0.07344388,  0.04771149,  0.0886896 , -0.09560982, -0.06605269,\n",
       "        0.09670116, -0.07804161,  0.03443696, -0.09693053, -0.09675052,\n",
       "       -0.0978159 ,  0.09246613,  0.09064306, -0.03115345,  0.09976891,\n",
       "        0.07518168,  0.09600248, -0.02502476,  0.06050119, -0.09992081,\n",
       "        0.07091616, -0.02410679,  0.09850984, -0.08482075, -0.02369144,\n",
       "       -0.07865445,  0.03303299,  0.04479694, -0.01118209,  0.00607729,\n",
       "       -0.09931464,  0.09836354,  0.06345432,  0.06485697, -0.02540703,\n",
       "        0.09839396, -0.09325194,  0.00104889,  0.0533358 , -0.07917981,\n",
       "        0.09203941, -0.07443949,  0.05108174, -0.05419624, -0.0984123 ,\n",
       "       -0.08552631,  0.09774892, -0.0997448 , -0.03287365,  0.0994132 ,\n",
       "        0.0678675 , -0.08788409, -0.04619691, -0.02930465, -0.07508024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_random_fourier_features(observation, action, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_data = collect_data(n_samples, policy_PPO, feature_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "action = env.action_space.sample()\n",
    "print(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = np.zeros(feature_dim)\n",
    "Q_lstd = policy_eval_LSTD(theta_init, offline_data)\n",
    "Q_BRM = policy_eval_BRM(theta_init, offline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_state_action(i, n_grid_points=num_grids):\n",
    "    \"\"\"\n",
    "    Maps index i (0 to 2*n_grid_points^4-1) to a state-action pair\n",
    "    \n",
    "    Returns:\n",
    "    - state: np.array of shape (4,)\n",
    "    - action: int (0 or 1)\n",
    "    \"\"\"\n",
    "    # State bounds\n",
    "    state_bounds = [\n",
    "        [-4.8, 4.8],     # cart position\n",
    "        [-10.0, 10.0],   # cart velocity\n",
    "        [-0.418, 0.418], # pole angle\n",
    "        [-10.0, 10.0]    # pole angular velocity\n",
    "    ]\n",
    "    \n",
    "    # Total states per dimension\n",
    "    n_states = n_grid_points**4\n",
    "    \n",
    "    # Determine action (0 for first half indices, 1 for second half)\n",
    "    action = 1 if i>= n_states else 0\n",
    "    \n",
    "    # Get state index (map back to state space)\n",
    "    state_idx = i % n_states\n",
    "    \n",
    "    # Convert to grid coordinates\n",
    "    idx_4 = state_idx % n_grid_points\n",
    "    idx_3 = (state_idx // n_grid_points) % n_grid_points\n",
    "    idx_2 = (state_idx // (n_grid_points**2)) % n_grid_points\n",
    "    idx_1 = state_idx // (n_grid_points**3)\n",
    "    \n",
    "    # Convert grid coordinates to actual state values\n",
    "    state = np.array([\n",
    "        np.linspace(state_bounds[0][0], state_bounds[0][1], n_grid_points)[idx_1],\n",
    "        np.linspace(state_bounds[1][0], state_bounds[1][1], n_grid_points)[idx_2],\n",
    "        np.linspace(state_bounds[2][0], state_bounds[2][1], n_grid_points)[idx_3],\n",
    "        np.linspace(state_bounds[3][0], state_bounds[3][1], n_grid_points)[idx_4]\n",
    "    ])\n",
    "    \n",
    "    return state, action\n",
    "\n",
    "# Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_evaluation_pairs(policy, num_grids = num_grids, n_episodes=100, max_steps=500):\n",
    "    \"\"\"\n",
    "    Estimates Q values using given policy for trajectories\n",
    "    \n",
    "    Args:\n",
    "        policy: Function that takes state and returns action\n",
    "        num_grids: Number of grid points per dimension\n",
    "        n_episodes: Number of episodes per state-action pair\n",
    "        max_steps: Maximum steps per episode\n",
    "        \n",
    "    Returns:\n",
    "        Q_vector: Estimated Q-values for each state-action pair\n",
    "    \"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "    total_pairs = 2 * num_grids**4\n",
    "    Q_vector = np.zeros(total_pairs)\n",
    "    \n",
    "    for i in tqdm(range(total_pairs)):\n",
    "        state, action = index_to_state_action(i, num_grids)\n",
    "        returns = []\n",
    "        \n",
    "        for _ in range(n_episodes):\n",
    "            env.reset()\n",
    "            env.state = state\n",
    "            \n",
    "            # Take specified initial action\n",
    "            next_state, reward, term, trunc, _ = env.step(action)\n",
    "            total_return = reward\n",
    "            \n",
    "            # Continue with policy-chosen actions\n",
    "            discount = gamma\n",
    "            curr_state = next_state\n",
    "            steps = 0\n",
    "            \n",
    "            while not (term or trunc) and steps < max_steps:\n",
    "                # Use policy to select action\n",
    "                curr_action = policy(curr_state)\n",
    "                curr_state, r, term, trunc, _ = env.step(curr_action)\n",
    "                total_return += discount * r\n",
    "                discount *= gamma\n",
    "                steps += 1\n",
    "                \n",
    "                if discount < 1e-10:\n",
    "                    break\n",
    "                    \n",
    "            returns.append(total_return)\n",
    "            \n",
    "        Q_vector[i] = np.mean(returns)\n",
    "    \n",
    "    env.close()\n",
    "    return Q_vector\n",
    "def loss_policy_evaluation(theta, Q_real, num_grids = num_grids):\n",
    "    loss = 0 \n",
    "    total_pairs = 2 * num_grids**4\n",
    "    for i in range(total_pairs):\n",
    "        state, action = index_to_state_action(i, num_grids)\n",
    "        Q_est_i = Q(state, action, theta)\n",
    "        loss += (Q_est_i- Q_real[i])**2\n",
    "    loss /= total_pairs\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [00:37<00:00,  4.37it/s]\n"
     ]
    }
   ],
   "source": [
    "Q_real = grid_evaluation_pairs(policy_mix(0.2))\n",
    "np.save(f\"Q_function_grid_3_mix_0.2.npy\", Q_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_real = np.load(f\"Q_function_grid_3.npy\")\n",
    "iter = int( n_samples / 50 )\n",
    "loss_LSTD = [0] * int(n_samples/ iter)\n",
    "loss_BRM = [0] * int(n_samples/ iter)\n",
    "total_pairs = 2 * num_grids**4\n",
    "\n",
    "l2_norm_diff_BRM_list = []\n",
    "l2_norm_diff_LSTD_list = []\n",
    "\n",
    "\n",
    "theta_lstd = np.zeros(feature_dim)\n",
    "theta_BRM = np.zeros(feature_dim)\n",
    "for m in range(iter, n_samples + 1, iter):\n",
    "    \n",
    "    offline_data = collect_data(iter, policy_PPO, feature_dim)\n",
    "    theta_lstd = policy_eval_LSTD(theta_lstd, offline_data)\n",
    "    theta_BRM = policy_eval_BRM(theta_BRM, offline_data)\n",
    "    loss_LSTD_m = loss_policy_evaluation(theta_lstd, Q_real)\n",
    "    loss_BRM_m = loss_policy_evaluation(theta_BRM, Q_real)\n",
    "\n",
    "    l2_norm_diff_LSTD_list.append(loss_LSTD_m)\n",
    "    l2_norm_diff_BRM_list.append(loss_BRM_m)\n",
    "    # print(len(l2_norm_diff_LSTD_list), len(l2_norm_diff_BRM_list))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(range(iter, n_samples + 1, iter), l2_norm_diff_BRM_list, label='BRM Loss', color='red')\n",
    "\n",
    "plt.plot(range(iter, n_samples + 1, iter), l2_norm_diff_LSTD_list, label='LSTD Loss', color='blue')\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.ylabel('L2 Norm Difference')\n",
    "# plt.yscale('log')\n",
    "plt.title('Loss Curves for BRM and LSTD')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('plot_image.pdf', bbox_inches='tight')          # Save as PDF\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "# Add this import to ensure proper initialization\n",
    "# import torch\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the Cartpole environment\n",
    "# env = gym.make('CartPole-v1')\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "obs = env.reset()[0]\n",
    "obs\n",
    "model = PPO.load(\"ppo_mountaincar\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(0, dtype=int64), None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = model.predict(obs)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.41714004, -0.00262002], dtype=float32), -1.0, False, False, {})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mountain car\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mountain-Car-env.py\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model_PPO = PPO.load(\"ppo_mountaincar\")\n",
    "\n",
    "env_name = 'MountainCar-v0'\n",
    "gym.make(env_name)\n",
    "\n",
    "n_samples = 1000\n",
    "feature_dim = 5 # Example feature dimension\n",
    "repeat = 1\n",
    "gamma = 0.9\n",
    "num_grids = 6\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env.reset()\n",
    "\n",
    "def policy_unif(s):\n",
    "  a = env.action_space.sample()\n",
    "  return a\n",
    "\n",
    "def policy_PPO(s):\n",
    "  action = model_PPO.predict(s)[0]\n",
    "  return action\n",
    "\n",
    "def rbf_random_fourier_features(state, action, feature_dim = feature_dim, length_scale=1.0):\n",
    "    np.random.seed(0)\n",
    "    state_array = np.array(state, dtype=np.float32).reshape(-1)\n",
    "    action_array = np.array([int(action)])\n",
    "    state_action = np.concatenate((state_array, action_array))\n",
    "    dim = state_action.shape[0]\n",
    "    \n",
    "    # Handle even/odd feature dimensions\n",
    "    if feature_dim % 2 == 0:\n",
    "        d_cos = d_sin = feature_dim // 2\n",
    "    else:\n",
    "        d_cos = (feature_dim + 1) // 2\n",
    "        d_sin = (feature_dim - 1) // 2\n",
    "    \n",
    "    omega = np.random.normal(scale=1.0/length_scale, size=(dim, d_cos))\n",
    "    bias = np.random.uniform(0, 2 * np.pi, size=d_cos)\n",
    "    z = state_action @ omega + bias\n",
    "    cos_features = np.cos(z)\n",
    "    sin_features = np.sin(z[:d_sin]) if d_sin > 0 else np.array([])\n",
    "    feature = np.sqrt(1.0 / feature_dim) * np.concatenate([cos_features, sin_features])\n",
    "    return feature\n",
    "\n",
    "def collect_trajectory(policy, feature_dim):\n",
    "    s0, _ = env.reset()\n",
    "    traj_list = [s0]\n",
    "    while True:\n",
    "        a0 = policy(s0)\n",
    "        phi_sa = rbf_random_fourier_features(s0, a0, feature_dim)\n",
    "        traj_list.append(phi_sa)\n",
    "        s1, r0,  terminated, truncated, _ = env.step(a0)\n",
    "        traj_list.append(r0)\n",
    "        traj_list.append(s1)\n",
    "        s0 = s1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    # print(len(traj_list))\n",
    "    return traj_list[:-1]  # removing the terminal state\n",
    "\n",
    "def collect_data(n, policy, feature_dim=feature_dim):\n",
    "    data = []\n",
    "    while len(data) < n:\n",
    "        trajectory = collect_trajectory(policy, feature_dim)\n",
    "        i = 0\n",
    "        while i < len(trajectory)-3:\n",
    "            state = trajectory[i]\n",
    "            action = policy(state)\n",
    "            phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "            reward = trajectory[i+2]\n",
    "            next_state = trajectory[i+3]\n",
    "            next_action = policy(next_state)\n",
    "            phi_sa_prime = rbf_random_fourier_features(next_state, next_action, feature_dim)\n",
    "            \n",
    "            data.append((phi_sa, reward, phi_sa_prime))\n",
    "            i += 3\n",
    "            if len(data) >= n:\n",
    "                break\n",
    "\n",
    "    return data[:n]  # Return exactly n samples as a single array\n",
    "\n",
    "def Q(state, action, theta,feature_dim=feature_dim):\n",
    "    phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    return np.dot(theta, phi_sa)\n",
    "\n",
    "def policy_eval_LSTD(theta_init,data, feature_dim=feature_dim, alpha=0.01):\n",
    "    '''Use TD(0) which converges to the solution of LSTD'''\n",
    "    theta_lstd = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        Q_sa = np.dot(theta_lstd, phi_sa)\n",
    "        Q_sa_prime = np.dot(theta_lstd, phi_sa_prime)\n",
    "        td_error = reward + gamma * Q_sa_prime - Q_sa\n",
    "        theta_lstd += alpha * td_error * phi_sa\n",
    "    \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_lstd, phi_sa)\n",
    "    \n",
    "    return theta_lstd\n",
    "\n",
    "def policy_eval_BRM(theta_init, data,  feature_dim=feature_dim, learning_rate=0.1):\n",
    "    theta_BRM = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        x_sa = phi_sa - gamma * phi_sa_prime\n",
    "        gradient = -2 * (reward - np.dot(x_sa, theta_BRM)) * x_sa\n",
    "        theta_BRM -= learning_rate * gradient\n",
    "        \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_BRM, phi_sa)\n",
    "    \n",
    "    return theta_BRM\n",
    "\n",
    "def index_to_state_action(i, n_grid_points=num_grids):\n",
    "    \"\"\"\n",
    "    Maps index i (0 to 3*n_grid_points^2-1) to a state-action pair\n",
    "    \n",
    "    Returns:\n",
    "    - state: np.array of shape (2,)\n",
    "    - action: int (0, 1, or 2)\n",
    "    \"\"\"\n",
    "    # State bounds\n",
    "    state_bounds = [\n",
    "        [-1.2, 0.6],     # position\n",
    "        [-0.07, 0.07]    # velocity\n",
    "    ]\n",
    "    \n",
    "    # Total states per dimension\n",
    "    n_states = n_grid_points**2\n",
    "    \n",
    "    # Determine action (0 for first third indices, 1 for second third, 2 for last third)\n",
    "    action = i // n_states\n",
    "    \n",
    "    # Get state index (map back to state space)\n",
    "    state_idx = i % n_states\n",
    "    \n",
    "    # Convert to grid coordinates\n",
    "    idx_2 = state_idx % n_grid_points\n",
    "    idx_1 = state_idx // n_grid_points\n",
    "    \n",
    "    # Convert grid coordinates to actual state values\n",
    "    state = np.array([\n",
    "        np.linspace(state_bounds[0][0], state_bounds[0][1], n_grid_points)[idx_1],\n",
    "        np.linspace(state_bounds[1][0], state_bounds[1][1], n_grid_points)[idx_2]\n",
    "    ])\n",
    "    \n",
    "    return state, action\n",
    "\n",
    "def grid_evaluation_pairs(policy, num_grids = num_grids, n_episodes=100, max_steps=200):\n",
    "    \"\"\"\n",
    "    Estimates Q values using given policy for trajectories\n",
    "    \n",
    "    Args:\n",
    "        policy: Function that takes state and returns action\n",
    "        num_grids: Number of grid points per dimension\n",
    "        n_episodes: Number of episodes per state-action pair\n",
    "        max_steps: Maximum steps per episode\n",
    "        \n",
    "    Returns:\n",
    "        Q_vector: Estimated Q-values for each state-action pair\n",
    "    \"\"\"\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    total_pairs = 3 * num_grids**2\n",
    "    Q_vector = np.zeros(total_pairs)\n",
    "    \n",
    "    for i in tqdm(range(total_pairs)):\n",
    "        state, action = index_to_state_action(i, num_grids)\n",
    "        returns = []\n",
    "        \n",
    "        for _ in range(n_episodes):\n",
    "            env.reset()\n",
    "            env.state = state\n",
    "            \n",
    "            # Take specified initial action\n",
    "            next_state, reward, term, trunc, _ = env.step(action)\n",
    "            total_return = reward\n",
    "            \n",
    "            # Continue with policy-chosen actions\n",
    "            discount = gamma\n",
    "            curr_state = next_state\n",
    "            steps = 0\n",
    "            \n",
    "            while not (term or trunc) and steps < max_steps:\n",
    "                # Use policy to select action\n",
    "                curr_action = policy(curr_state)\n",
    "                curr_state, r, term, trunc, _ = env.step(curr_action)\n",
    "                total_return += discount * r\n",
    "                discount *= gamma\n",
    "                steps += 1\n",
    "                \n",
    "                if discount < 1e-10:\n",
    "                    break\n",
    "                    \n",
    "            returns.append(total_return)\n",
    "            \n",
    "        Q_vector[i] = np.mean(returns)\n",
    "    \n",
    "    env.close()\n",
    "    return Q_vector\n",
    "\n",
    "def loss_policy_evaluation(theta, Q_real, num_grids = num_grids):\n",
    "    loss = 0 \n",
    "    total_pairs = 2 * num_grids**4\n",
    "    for i in range(total_pairs):\n",
    "        state, action = index_to_state_action(i, num_grids)\n",
    "        Q_est_i = Q(state, action, theta)\n",
    "        loss += (Q_est_i- Q_real[i])**2\n",
    "    loss /= total_pairs\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [09:36<00:00,  5.34s/it]\n"
     ]
    }
   ],
   "source": [
    "Q_real = grid_evaluation_pairs(policy_PPO)\n",
    "np.save(f\"Q_function_Mountain_car_grid_6.npy\", Q_real)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
