{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def policy_unif(s):\n",
    "  a = env.action_space.sample()\n",
    "  return a\n",
    "def best_policy(s):\n",
    "  policy_map = {\n",
    "    0: 1, 1: 2, 2: 1, 3: 0,\n",
    "    4: 1, 5: 1, 6: 1, 7: 2,\n",
    "    8: 2, 9: 2, 10: 1, 11: 2,\n",
    "    12: 1, 13: 2, 14: 2, 15: 2\n",
    "  }\n",
    "  return policy_map.get(s, env.action_space.sample())\n",
    "\n",
    "def best_policy_rand(s):\n",
    "  random_value = np.random.rand()\n",
    "  if random_value < 0.9:\n",
    "    return best_policy(s)\n",
    "  else:\n",
    "    return policy_unif(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.0\n",
      "False\n",
      "False\n",
      "{'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "print(observation)\n",
    "print(reward)\n",
    "print(terminated)\n",
    "print(truncated)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(policy):\n",
    "  s0 = env.reset()[0]\n",
    "  traj_list = [s0]\n",
    "  while True:\n",
    "    a0 = policy(s0)\n",
    "    traj_list.append(a0)\n",
    "    s1, r0, terminated, truncated, _ = env.step(a0)\n",
    "    traj_list.append(r0)\n",
    "    traj_list.append(s1)\n",
    "    s0 = s1\n",
    "    if terminated or truncated:\n",
    "      break\n",
    "  return traj_list[:-1] #removing the terminal state\n",
    "\n",
    "def collect_trajectory_s_a(policy,s0,a0):\n",
    "  env.reset()\n",
    "  env.unwrapped.s = s0\n",
    "  traj_list = [s0,a0]\n",
    "  s1, r0, terminated, truncated, _ = env.step(a0)\n",
    "  traj_list.append(r0)\n",
    "  traj_list.append(s1)\n",
    "  s0=s1\n",
    "  if terminated or truncated:\n",
    "    return traj_list[:-1]\n",
    "  \n",
    "  while True:\n",
    "    a0 = policy(s0)\n",
    "    traj_list.append(a0)\n",
    "    s1, r0, terminated, truncated, _ = env.step(a0)\n",
    "    traj_list.append(r0)\n",
    "    traj_list.append(s1)\n",
    "    s0 = s1\n",
    "    if terminated or truncated:\n",
    "      break\n",
    "  return traj_list[:-1] #removing the terminal state\n",
    "\n",
    "def compute_return(traj,gamma=0.99):\n",
    "  if traj==[]:\n",
    "    return 0\n",
    "  else:\n",
    "    return traj[2]+gamma*compute_return(traj[3:],gamma)\n",
    "  \n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "def policy_evaluation(policy):\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for state in range(n_states):\n",
    "        for action in range(n_actions):\n",
    "            traj = collect_trajectory_s_a(policy, state, action)\n",
    "            Q[state, action] = compute_return(traj)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0.0, 4, 1, 0.0, 8, 1, 0.0]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "traj = collect_trajectory(best_policy_rand)\n",
    "print(traj)\n",
    "print(compute_return(traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 1, 0.0, 14, 2, 1.0]\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "traj = collect_trajectory_s_a(best_policy,14,1)\n",
    "print(traj)\n",
    "print(compute_return(traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94148015, 0.95099005, 0.95099005, 0.94148015],\n",
       "       [0.94148015, 0.        , 0.96059601, 0.95099005],\n",
       "       [0.95099005, 0.970299  , 0.95099005, 0.96059601],\n",
       "       [0.96059601, 0.        , 0.95099005, 0.95099005],\n",
       "       [0.95099005, 0.96059601, 0.        , 0.94148015],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9801    , 0.        , 0.96059601],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.96059601, 0.        , 0.970299  , 0.95099005],\n",
       "       [0.96059601, 0.9801    , 0.9801    , 0.        ],\n",
       "       [0.970299  , 0.99      , 0.        , 0.970299  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9801    , 0.99      , 0.970299  ],\n",
       "       [0.9801    , 0.99      , 1.        , 0.9801    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_evaluation(best_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "s1, r0, terminated, truncated, _ = env.step(2)\n",
    "print(s1)\n",
    "print(env.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.27it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# frozen-lake.py\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "n_samples = 1000\n",
    "feature_dim = 10 # Example feature dimension\n",
    "repeat = 1\n",
    "gamma = 0.99\n",
    "env_name = 'FrozenLake-v1'\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "def policy_unif(s):\n",
    "  a = env.action_space.sample()\n",
    "  return a\n",
    "\n",
    "def best_policy(s):\n",
    "  policy_map = {\n",
    "    0: 1, 1: 2, 2: 1, 3: 0,\n",
    "    4: 1, 5: 1, 6: 1, 7: 2,\n",
    "    8: 2, 9: 2, 10: 1, 11: 2,\n",
    "    12: 1, 13: 2, 14: 2, 15: 2\n",
    "  }\n",
    "  return policy_map.get(s, env.action_space.sample())\n",
    "\n",
    "def best_policy_rand(s):\n",
    "  random_value = np.random.rand()\n",
    "  if random_value < 0.9:\n",
    "    return best_policy(s)\n",
    "  else:\n",
    "    return policy_unif(s)\n",
    "\n",
    "def rbf_random_fourier_features(state, action, feature_dim = feature_dim, length_scale=1.0):\n",
    "    # return fourier_features(state, action, feature_dim)\n",
    "    np.random.seed(0)\n",
    "    state_array = np.array(state, dtype=np.float32).reshape(-1)\n",
    "    action_array = np.array([int(action)])\n",
    "    state_action = np.concatenate((state_array, action_array))\n",
    "    dim = state_action.shape[0]\n",
    "    \n",
    "    # Handle even/odd feature dimensions\n",
    "    if feature_dim % 2 == 0:\n",
    "        d_cos = d_sin = feature_dim // 2\n",
    "    else:\n",
    "        d_cos = (feature_dim + 1) // 2\n",
    "        d_sin = (feature_dim - 1) // 2\n",
    "    \n",
    "    omega = np.random.normal(scale=1.0/length_scale, size=(dim, d_cos))\n",
    "    bias = np.random.uniform(0, 2 * np.pi, size=d_cos)\n",
    "    z = state_action @ omega + bias\n",
    "    cos_features = np.cos(z)\n",
    "    sin_features = np.sin(z[:d_sin]) if d_sin > 0 else np.array([])\n",
    "    feature = np.sqrt(1.0 / feature_dim) * np.concatenate([cos_features, sin_features])\n",
    "    return feature\n",
    "\n",
    "def collect_trajectory(policy, feature_dim):\n",
    "    s0, _ = env.reset()\n",
    "    traj_list = [s0]\n",
    "    while True:\n",
    "        a0 = policy(s0)\n",
    "        phi_sa = rbf_random_fourier_features(s0, a0, feature_dim)\n",
    "        traj_list.append(phi_sa)\n",
    "        s1, r0,  terminated, truncated, _ = env.step(a0)\n",
    "        traj_list.append(r0)\n",
    "        traj_list.append(s1)\n",
    "        s0 = s1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    # print(len(traj_list))\n",
    "    return traj_list[:-1]  # removing the terminal state\n",
    "\n",
    "def collect_data(n, policy_to_gen_data, policy_to_eval, feature_dim=feature_dim):\n",
    "    data = []\n",
    "    while len(data) < n:\n",
    "        trajectory = collect_trajectory(policy_to_gen_data, feature_dim)\n",
    "        i = 0\n",
    "        while i < len(trajectory)-3:\n",
    "            state = trajectory[i]\n",
    "            \n",
    "            phi_sa = trajectory[i+1]\n",
    "            reward = trajectory[i+2]\n",
    "            next_state = trajectory[i+3]\n",
    "            next_action = policy_to_eval(next_state)\n",
    "            phi_sa_prime = rbf_random_fourier_features(next_state, next_action, feature_dim)\n",
    "            \n",
    "            data.append((phi_sa, reward, phi_sa_prime))\n",
    "            i += 3\n",
    "            if len(data) >= n:\n",
    "                break\n",
    "\n",
    "    return data[:n]  # Return exactly n samples as a single array\n",
    "\n",
    "def Q(state, action, theta,feature_dim=feature_dim):\n",
    "    phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    return np.dot(theta, phi_sa)\n",
    "\n",
    "def policy_eval_LSTD(theta_init,data, feature_dim=feature_dim, alpha=0.01):\n",
    "    '''Use TD(0) which converges to the solution of LSTD'''\n",
    "    theta_lstd = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        Q_sa = np.dot(theta_lstd, phi_sa)\n",
    "        Q_sa_prime = np.dot(theta_lstd, phi_sa_prime)\n",
    "        td_error = reward + gamma * Q_sa_prime - Q_sa\n",
    "        theta_lstd += alpha * td_error * phi_sa\n",
    "    \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_lstd, phi_sa)\n",
    "    \n",
    "    return theta_lstd\n",
    "\n",
    "def policy_eval_BRM(theta_init, data,  feature_dim=feature_dim, learning_rate=0.1):\n",
    "    theta_BRM = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        x_sa = phi_sa - gamma * phi_sa_prime\n",
    "        gradient = -2 * (reward - np.dot(x_sa, theta_BRM)) * x_sa\n",
    "        theta_BRM -= learning_rate * gradient\n",
    "        \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_BRM, phi_sa)\n",
    "    \n",
    "    return theta_BRM\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "def compute_return(traj):\n",
    "  if traj==[]:\n",
    "    return 0\n",
    "  else:\n",
    "    return traj[2]+gamma*compute_return(traj[3:])\n",
    "  \n",
    "def collect_trajectory_s_a(policy,s0,a0):\n",
    "  env.reset()\n",
    "  env.unwrapped.s = s0\n",
    "  traj_list = [s0,a0]\n",
    "  s1, r0, terminated, truncated, _ = env.step(a0)\n",
    "  traj_list.append(r0)\n",
    "  traj_list.append(s1)\n",
    "  s0=s1\n",
    "  if terminated or truncated:\n",
    "    return traj_list[:-1]\n",
    "  \n",
    "  while True:\n",
    "    a0 = policy(s0)\n",
    "    traj_list.append(a0)\n",
    "    s1, r0, terminated, truncated, _ = env.step(a0)\n",
    "    traj_list.append(r0)\n",
    "    traj_list.append(s1)\n",
    "    s0 = s1\n",
    "    if terminated or truncated:\n",
    "      break\n",
    "  return traj_list[:-1] #removing the terminal state\n",
    "\n",
    "def compute_Q_real(policy):\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for state in range(n_states):\n",
    "        for action in range(n_actions):\n",
    "            traj = collect_trajectory_s_a(policy, state, action)\n",
    "            Q[state, action] = compute_return(traj)\n",
    "    return Q\n",
    "\n",
    "# Calculate Q_real for the best policy\n",
    "Q_real = compute_Q_real(best_policy)\n",
    "\n",
    "def loss_policy_evaluation(theta, Q_real):\n",
    "    loss = 0 \n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    for state in range(n_states):\n",
    "        for action in range(n_actions):\n",
    "            Q_est_i = Q(state, action, theta)\n",
    "            loss += (Q_est_i- Q_real[state,action])**2\n",
    "    loss /= n_states * n_actions\n",
    "    return loss\n",
    "\n",
    "\n",
    "iter = int( n_samples / 50 )\n",
    "loss_LSTD = [0] * int(n_samples/ iter)\n",
    "loss_BRM = [0] * int(n_samples/ iter)\n",
    "\n",
    "for _ in tqdm(range(repeat)):\n",
    "    l2_norm_diff_BRM_list = []\n",
    "    l2_norm_diff_LSTD_list = []\n",
    "    theta_lstd = np.zeros(feature_dim)\n",
    "    theta_BRM = np.zeros(feature_dim)\n",
    "    for m in range(iter, n_samples + 1, iter):\n",
    "        \n",
    "        offline_data = collect_data(iter, best_policy_rand,best_policy, feature_dim)\n",
    "        theta_lstd = policy_eval_LSTD(theta_lstd, offline_data)\n",
    "        theta_BRM = policy_eval_BRM(theta_BRM, offline_data)\n",
    "        loss_LSTD_m = loss_policy_evaluation(theta_lstd, Q_real)\n",
    "        loss_BRM_m = loss_policy_evaluation(theta_BRM, Q_real)\n",
    "\n",
    "        l2_norm_diff_LSTD_list.append(loss_LSTD_m)\n",
    "        l2_norm_diff_BRM_list.append(loss_BRM_m)\n",
    "    # print(len(l2_norm_diff_LSTD_list), len(l2_norm_diff_BRM_list))\n",
    "    loss_LSTD = [a + b for a, b in zip(loss_LSTD, l2_norm_diff_LSTD_list)]\n",
    "    loss_BRM = [a + b for a, b in zip(loss_BRM, l2_norm_diff_BRM_list)]\n",
    "loss_LSTD = [value / repeat for value in loss_LSTD]\n",
    "loss_BRM = [value / repeat for value in loss_BRM]\n",
    "# loss_oracle = loss_policy_evaluation(theta_oracle, Q_real)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# plt.axhline(y=loss_oracle, color='green', linestyle='--', label='Oracle Loss')\n",
    "plt.plot(range(iter, n_samples + 1, iter), loss_BRM, label='BRM Loss', color='red')\n",
    "plt.plot(range(iter, n_samples + 1, iter), loss_LSTD, label='LSTD Loss', color='blue')\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.ylabel('L2 Norm Difference')\n",
    "# plt.yscale('log')\n",
    "plt.title(f'Loss Curves for BRM and LSTD in {env_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig(f'plot_image_env_{env_name}_n_samples_{n_samples}_feature_dim_{feature_dim}_repeat_{repeat}_gamma_{gamma}.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
