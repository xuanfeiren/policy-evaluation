{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PPO = PPO.load(\"ppo_cartpole\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "feature_dim = 10  # Example feature dimension\n",
    "repeat = 1\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00022417,  0.01703972, -0.03923923, -0.02722882], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('MlpPolicy', env, verbose=1)\n",
    "# model.learn(total_timesteps=100000)\n",
    "# model.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "obs\n",
    "action = model_PPO.predict(obs)[0]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_unif(s):\n",
    "  a = env.action_space.sample()\n",
    "  return a\n",
    "\n",
    "def policy_PPO(s):\n",
    "  action = model_PPO.predict(s)[0]\n",
    "  return action\n",
    "\n",
    "def rbf_random_fourier_features(state, action, feature_dim = feature_dim, length_scale=1.0):\n",
    "    np.random.seed(0)\n",
    "    state_array = np.array(state[0], dtype=np.float32).reshape(-1)\n",
    "    action_array = np.array([float(action)])\n",
    "    state_action = np.concatenate((state_array, action_array))\n",
    "    dim = state_action.shape[0]\n",
    "    \n",
    "    # Handle even/odd feature dimensions\n",
    "    if feature_dim % 2 == 0:\n",
    "        d_cos = d_sin = feature_dim // 2\n",
    "    else:\n",
    "        d_cos = (feature_dim + 1) // 2\n",
    "        d_sin = (feature_dim - 1) // 2\n",
    "    \n",
    "    omega = np.random.normal(scale=1.0/length_scale, size=(dim, d_cos))\n",
    "    bias = np.random.uniform(0, 2 * np.pi, size=d_cos)\n",
    "    z = state_action @ omega + bias\n",
    "    cos_features = np.cos(z)\n",
    "    sin_features = np.sin(z[:d_sin]) if d_sin > 0 else np.array([])\n",
    "    feature = np.sqrt(1.0 / feature_dim) * np.concatenate([cos_features, sin_features])\n",
    "    return feature\n",
    "\n",
    "def collect_trajectory(policy, feature_dim):\n",
    "    s0, _ = env.reset()\n",
    "    traj_list = [s0]\n",
    "    while True:\n",
    "        a0 = policy(s0)\n",
    "        phi_sa = rbf_random_fourier_features(s0, a0, feature_dim)\n",
    "        traj_list.append(phi_sa)\n",
    "        s1, r0,  terminated, truncated, _ = env.step(a0)\n",
    "        traj_list.append(r0)\n",
    "        traj_list.append(s1)\n",
    "        s0 = s1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    # print(len(traj_list))\n",
    "    return traj_list[:-1]  # removing the terminal state\n",
    "\n",
    "def collect_data(n, policy, feature_dim=feature_dim):\n",
    "    data = []\n",
    "    while len(data) < n:\n",
    "        trajectory = collect_trajectory(policy, feature_dim)\n",
    "        i = 0\n",
    "        while i < len(trajectory)-3:\n",
    "            state = trajectory[i]\n",
    "            action = policy(state)\n",
    "            phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "            reward = trajectory[i+2]\n",
    "            next_state = trajectory[i+3]\n",
    "            next_action = policy(next_state)\n",
    "            phi_sa_prime = rbf_random_fourier_features(next_state, next_action, feature_dim)\n",
    "            \n",
    "            data.append((phi_sa, reward, phi_sa_prime))\n",
    "            i += 3\n",
    "            if len(data) >= n:\n",
    "                break\n",
    "\n",
    "    return data[:n]  # Return exactly n samples as a single array\n",
    "\n",
    "def Q(state, action, theta,feature_dim=feature_dim):\n",
    "    phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    return np.dot(theta, phi_sa)\n",
    "\n",
    "def policy_eval_LSTD(theta_init,data, feature_dim=feature_dim, alpha=0.01):\n",
    "    '''Use TD(0) which converges to the solution of LSTD'''\n",
    "    theta_lstd = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        Q_sa = np.dot(theta_lstd, phi_sa)\n",
    "        Q_sa_prime = np.dot(theta_lstd, phi_sa_prime)\n",
    "        td_error = reward + gamma * Q_sa_prime - Q_sa\n",
    "        theta_lstd += alpha * td_error * phi_sa\n",
    "    \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_lstd, phi_sa)\n",
    "    \n",
    "    return theta_lstd\n",
    "\n",
    "def policy_eval_BRM(theta_init, data,  feature_dim=feature_dim, learning_rate=0.1):\n",
    "    theta_BRM = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        x_sa = phi_sa - gamma * phi_sa_prime\n",
    "        gradient = -2 * (reward - np.dot(x_sa, theta_BRM)) * x_sa\n",
    "        theta_BRM -= learning_rate * gradient\n",
    "        \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_BRM, phi_sa)\n",
    "    \n",
    "    return theta_BRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_data = collect_data(n_samples, policy_PPO, feature_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00803419 -0.04799614 -0.04166585  0.03667999] 1\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "action = env.action_space.sample()\n",
    "print(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = np.zeros(feature_dim)\n",
    "Q_lstd = policy_eval_LSTD(theta_init, offline_data)\n",
    "Q_BRM = policy_eval_BRM(theta_init, offline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = int( n / 50 )\n",
    "loss_LSTD = [0] * int(n / iter)\n",
    "loss_BRM = [0] * int(n / iter)\n",
    "\n",
    "l2_norm_diff_BRM_list = []\n",
    "l2_norm_diff_LSTD_list = []\n",
    "l2_norm_diff_FQI_list = []\n",
    "\n",
    "theta_init = np.zeros(feature_dim)\n",
    "theta_lstd = np.copy(theta_init)\n",
    "theta_BRM = np.copy(theta_init)\n",
    "for m in range(iter, n + 1, iter):\n",
    "    \n",
    "    offline_data = collect_data(iter, policy_PPO, feature_dim)\n",
    "    theta_lstd = policy_eval_LSTD(theta_lstd, offline_data)\n",
    "    theta_BRM = policy_eval_BRM(theta_BRM, offline_data)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rxf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
