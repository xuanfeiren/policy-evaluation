{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PPO = PPO.load(\"ppo_cartpole\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "feature_dim = 10  # Example feature dimension\n",
    "repeat = 1\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01502095,  0.03861119, -0.04368586,  0.01309333], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('MlpPolicy', env, verbose=1)\n",
    "# model.learn(total_timesteps=100000)\n",
    "# model.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "obs\n",
    "action = model_PPO.predict(obs)[0]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_unif(s):\n",
    "  a = env.action_space.sample()\n",
    "  return a\n",
    "\n",
    "def policy_PPO(s):\n",
    "  action = model_PPO.predict(s)[0]\n",
    "  return action\n",
    "\n",
    "def rbf_random_fourier_features(state, action, feature_dim = feature_dim, length_scale=1.0):\n",
    "    np.random.seed(0)\n",
    "    state_array = np.array(state[0], dtype=np.float32).reshape(-1)\n",
    "    action_array = np.array([float(action)])\n",
    "    state_action = np.concatenate((state_array, action_array))\n",
    "    dim = state_action.shape[0]\n",
    "    \n",
    "    # Handle even/odd feature dimensions\n",
    "    if feature_dim % 2 == 0:\n",
    "        d_cos = d_sin = feature_dim // 2\n",
    "    else:\n",
    "        d_cos = (feature_dim + 1) // 2\n",
    "        d_sin = (feature_dim - 1) // 2\n",
    "    \n",
    "    omega = np.random.normal(scale=1.0/length_scale, size=(dim, d_cos))\n",
    "    bias = np.random.uniform(0, 2 * np.pi, size=d_cos)\n",
    "    z = state_action @ omega + bias\n",
    "    cos_features = np.cos(z)\n",
    "    sin_features = np.sin(z[:d_sin]) if d_sin > 0 else np.array([])\n",
    "    feature = np.sqrt(1.0 / feature_dim) * np.concatenate([cos_features, sin_features])\n",
    "    return feature\n",
    "\n",
    "def collect_trajectory(policy, feature_dim):\n",
    "    s0, _ = env.reset()\n",
    "    traj_list = [s0]\n",
    "    while True:\n",
    "        a0 = policy(s0)\n",
    "        phi_sa = rbf_random_fourier_features(s0, a0, feature_dim)\n",
    "        traj_list.append(phi_sa)\n",
    "        s1, r0,  terminated, truncated, _ = env.step(a0)\n",
    "        traj_list.append(r0)\n",
    "        traj_list.append(s1)\n",
    "        s0 = s1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    # print(len(traj_list))\n",
    "    return traj_list[:-1]  # removing the terminal state\n",
    "\n",
    "def collect_data(n, policy, feature_dim=feature_dim):\n",
    "    data = []\n",
    "    while len(data) < n:\n",
    "        trajectory = collect_trajectory(policy, feature_dim)\n",
    "        i = 0\n",
    "        while i < len(trajectory)-3:\n",
    "            state = trajectory[i]\n",
    "            action = policy(state)\n",
    "            phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "            reward = trajectory[i+2]\n",
    "            next_state = trajectory[i+3]\n",
    "            next_action = policy(next_state)\n",
    "            phi_sa_prime = rbf_random_fourier_features(next_state, next_action, feature_dim)\n",
    "            \n",
    "            data.append((phi_sa, reward, phi_sa_prime))\n",
    "            i += 3\n",
    "            if len(data) >= n:\n",
    "                break\n",
    "\n",
    "    return data[:n]  # Return exactly n samples as a single array\n",
    "\n",
    "def Q(state, action, theta,feature_dim=feature_dim):\n",
    "    phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    return np.dot(theta, phi_sa)\n",
    "\n",
    "def policy_eval_LSTD(theta_init,data, feature_dim=feature_dim, alpha=0.01):\n",
    "    '''Use TD(0) which converges to the solution of LSTD'''\n",
    "    theta_lstd = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        Q_sa = np.dot(theta_lstd, phi_sa)\n",
    "        Q_sa_prime = np.dot(theta_lstd, phi_sa_prime)\n",
    "        td_error = reward + gamma * Q_sa_prime - Q_sa\n",
    "        theta_lstd += alpha * td_error * phi_sa\n",
    "    \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_lstd, phi_sa)\n",
    "    \n",
    "    return theta_lstd\n",
    "\n",
    "def policy_eval_BRM(theta_init, data,  feature_dim=feature_dim, learning_rate=0.1):\n",
    "    theta_BRM = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        x_sa = phi_sa - gamma * phi_sa_prime\n",
    "        gradient = -2 * (reward - np.dot(x_sa, theta_BRM)) * x_sa\n",
    "        theta_BRM -= learning_rate * gradient\n",
    "        \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_BRM, phi_sa)\n",
    "    \n",
    "    return theta_BRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_data = collect_data(n_samples, policy_PPO, feature_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01382144  0.01666469  0.04897724  0.01250008] 1\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "action = env.action_space.sample()\n",
    "print(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.231422191006378 5.327932805150623\n"
     ]
    }
   ],
   "source": [
    "theta_init = np.zeros(feature_dim)\n",
    "Q_lstd = policy_eval_LSTD(theta_init, offline_data)\n",
    "Q_BRM = policy_eval_BRM(theta_init, offline_data)\n",
    "estimated_value_lstd_gradient = Q_lstd(state, action)\n",
    "estimated_value_BRM = Q_BRM(state, action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_state_action(i, n_grid_points=5):\n",
    "    \"\"\"\n",
    "    Maps index i (0 to 2*n_grid_points^4-1) to a state-action pair\n",
    "    \n",
    "    Returns:\n",
    "    - state: np.array of shape (4,)\n",
    "    - action: int (0 or 1)\n",
    "    \"\"\"\n",
    "    # State bounds\n",
    "    state_bounds = [\n",
    "        [-4.8, 4.8],     # cart position\n",
    "        [-10.0, 10.0],   # cart velocity\n",
    "        [-0.418, 0.418], # pole angle\n",
    "        [-10.0, 10.0]    # pole angular velocity\n",
    "    ]\n",
    "    \n",
    "    # Total states per dimension\n",
    "    n_states = n_grid_points**4\n",
    "    \n",
    "    # Determine action (0 for first half indices, 1 for second half)\n",
    "    action = 1 if i >= n_states else 0\n",
    "    \n",
    "    # Get state index (map back to state space)\n",
    "    state_idx = i % n_states\n",
    "    \n",
    "    # Convert to grid coordinates\n",
    "    idx_4 = state_idx % n_grid_points\n",
    "    idx_3 = (state_idx // n_grid_points) % n_grid_points\n",
    "    idx_2 = (state_idx // (n_grid_points**2)) % n_grid_points\n",
    "    idx_1 = state_idx // (n_grid_points**3)\n",
    "    \n",
    "    # Convert grid coordinates to actual state values\n",
    "    state = np.array([\n",
    "        np.linspace(state_bounds[0][0], state_bounds[0][1], n_grid_points)[idx_1],\n",
    "        np.linspace(state_bounds[1][0], state_bounds[1][1], n_grid_points)[idx_2],\n",
    "        np.linspace(state_bounds[2][0], state_bounds[2][1], n_grid_points)[idx_3],\n",
    "        np.linspace(state_bounds[3][0], state_bounds[3][1], n_grid_points)[idx_4]\n",
    "    ])\n",
    "    \n",
    "    return state, action\n",
    "\n",
    "# Example usage:\n",
    "total_pairs = 2 * 5**4  # 1250 total state-action pairs\n",
    "for i in range(total_pairs):\n",
    "    state, action = index_to_state_action(i)\n",
    "    # Use these for Q-value estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  Q_values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m Q_values \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_grid_evaluation_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(Q_values\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (1250, 1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[62], line 54\u001b[0m, in \u001b[0;36mcreate_grid_evaluation_pairs\u001b[0;34m(n_grid_points)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Follow policy until episode end\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     policy_action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_PPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     curr_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(policy_action)\n\u001b[1;32m     56\u001b[0m     G \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (gamma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m, in \u001b[0;36mpolicy_PPO\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpolicy_PPO\u001b[39m(s):\n\u001b[0;32m----> 6\u001b[0m   action \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_PPO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[0;32m~/miniconda3/envs/rxf/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rxf/lib/python3.12/site-packages/stable_baselines3/common/policies.py:368\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[1;32m    370\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc, assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rxf/lib/python3.12/site-packages/stable_baselines3/common/policies.py:717\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rxf/lib/python3.12/site-packages/stable_baselines3/common/distributions.py:89\u001b[0m, in \u001b[0;36mDistribution.get_actions\u001b[0;34m(self, deterministic)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode()\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rxf/lib/python3.12/site-packages/stable_baselines3/common/distributions.py:298\u001b[0m, in \u001b[0;36mCategoricalDistribution.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rxf/lib/python3.12/site-packages/torch/distributions/categorical.py:134\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    132\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    133\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 134\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_grid_evaluation_pairs(n_grid_points=5):\n",
    "    \"\"\"\n",
    "    Create uniform grid of state-action pairs and estimate Q-values\n",
    "    \"\"\"\n",
    "    # Step 1: Define state bounds\n",
    "    state_bounds = [\n",
    "        [-4.8, 4.8],     # cart position\n",
    "        [-10.0, 10.0],   # cart velocity \n",
    "        [-0.418, 0.418], # pole angle\n",
    "        [-10.0, 10.0]    # pole angular velocity\n",
    "    ]\n",
    "    \n",
    "    # Step 2: Create grid points for each dimension\n",
    "    grids = []\n",
    "    for bound in state_bounds:\n",
    "        grids.append(np.linspace(bound[0], bound[1], n_grid_points))\n",
    "    \n",
    "    # Step 3: Create all combinations using meshgrid\n",
    "    cart_pos, cart_vel, pole_ang, pole_vel = np.meshgrid(*grids)\n",
    "    \n",
    "    # Step 4: Reshape to get state matrix (n_grid_points^4 x 4)\n",
    "    states = np.column_stack((\n",
    "        cart_pos.flatten(),\n",
    "        cart_vel.flatten(),\n",
    "        pole_ang.flatten(),\n",
    "        pole_vel.flatten()\n",
    "    ))\n",
    "    \n",
    "    # Step 5: Create state-action pairs (duplicate states for both actions)\n",
    "    n_states = len(states)\n",
    "    \n",
    "    # Step 6: Estimate Q-values using Monte Carlo sampling\n",
    "    n_eval_episodes = 50\n",
    "    gamma = 0.99\n",
    "    Q_values = np.zeros(2 * n_states)\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    for i in range(2*n_states):\n",
    "        returns = []\n",
    "        state, action = index_to_state_action(i)\n",
    "        \n",
    "        for _ in range(n_eval_episodes):\n",
    "            env.reset()\n",
    "            env.state = state\n",
    "            next_state, reward, terminated, truncated, _ = env.step(int(action))\n",
    "            \n",
    "            # First reward from taking action a\n",
    "            G = reward\n",
    "            curr_state = next_state\n",
    "            step = 0\n",
    "            \n",
    "            # Follow policy until episode end\n",
    "            while not terminated and not truncated and step < 500:\n",
    "                policy_action = policy_PPO(curr_state)\n",
    "                curr_state, reward, terminated, truncated, _ = env.step(policy_action)\n",
    "                G += (gamma ** (step + 1)) * reward\n",
    "                step += 1\n",
    "            \n",
    "            returns.append(G)\n",
    "        \n",
    "        Q_values[i] = np.mean(returns)\n",
    "    \n",
    "    env.close()\n",
    "    return  Q_values.reshape(-1, 1)\n",
    "\n",
    "# Usage\n",
    "Q_values = create_grid_evaluation_pairs()\n",
    "print(Q_values.shape)  # (1250, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = int( n / 50 )\n",
    "loss_LSTD = [0] * int(n / iter)\n",
    "loss_BRM = [0] * int(n / iter)\n",
    "\n",
    "l2_norm_diff_BRM_list = []\n",
    "l2_norm_diff_LSTD_list = []\n",
    "l2_norm_diff_FQI_list = []\n",
    "\n",
    "theta_init = np.zeros(feature_dim)\n",
    "theta_lstd = np.copy(theta_init)\n",
    "theta_BRM = np.copy(theta_init)\n",
    "for m in range(iter, n + 1, iter):\n",
    "    \n",
    "    offline_data = collect_data(iter, policy_PPO, feature_dim)\n",
    "    theta_lstd = policy_eval_LSTD(theta_lstd, offline_data)\n",
    "    theta_BRM = policy_eval_BRM(theta_BRM, offline_data)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rxf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
