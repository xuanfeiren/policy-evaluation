{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PPO = PPO.load(\"ppo_cartpole\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "feature_dim = 10  # Example feature dimension\n",
    "repeat = 1\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02919586,  0.01587004, -0.03414124,  0.03957219], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('MlpPolicy', env, verbose=1)\n",
    "# model.learn(total_timesteps=100000)\n",
    "# model.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1, dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "obs\n",
    "action = model_PPO.predict(obs)[0]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_unif(s):\n",
    "  a = env.action_space.sample()\n",
    "  return a\n",
    "\n",
    "def policy_PPO(s):\n",
    "  action = model_PPO.predict(s)[0]\n",
    "  return action\n",
    "\n",
    "def rbf_random_fourier_features(state, action, feature_dim = feature_dim, length_scale=1.0):\n",
    "    np.random.seed(0)\n",
    "    state_array = np.array(state[0], dtype=np.float32).reshape(-1)\n",
    "    action_array = np.array([float(action)])\n",
    "    state_action = np.concatenate((state_array, action_array))\n",
    "    dim = state_action.shape[0]\n",
    "    \n",
    "    # Handle even/odd feature dimensions\n",
    "    if feature_dim % 2 == 0:\n",
    "        d_cos = d_sin = feature_dim // 2\n",
    "    else:\n",
    "        d_cos = (feature_dim + 1) // 2\n",
    "        d_sin = (feature_dim - 1) // 2\n",
    "    \n",
    "    omega = np.random.normal(scale=1.0/length_scale, size=(dim, d_cos))\n",
    "    bias = np.random.uniform(0, 2 * np.pi, size=d_cos)\n",
    "    z = state_action @ omega + bias\n",
    "    cos_features = np.cos(z)\n",
    "    sin_features = np.sin(z[:d_sin]) if d_sin > 0 else np.array([])\n",
    "    feature = np.sqrt(1.0 / feature_dim) * np.concatenate([cos_features, sin_features])\n",
    "    return feature\n",
    "\n",
    "def collect_trajectory(policy, feature_dim):\n",
    "    s0, _ = env.reset()\n",
    "    traj_list = [s0]\n",
    "    while True:\n",
    "        a0 = policy(s0)\n",
    "        phi_sa = rbf_random_fourier_features(s0, a0, feature_dim)\n",
    "        traj_list.append(phi_sa)\n",
    "        s1, r0,  terminated, truncated, _ = env.step(a0)\n",
    "        traj_list.append(r0)\n",
    "        traj_list.append(s1)\n",
    "        s0 = s1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    # print(len(traj_list))\n",
    "    return traj_list[:-1]  # removing the terminal state\n",
    "\n",
    "def collect_data(n, policy, feature_dim=feature_dim):\n",
    "    data = []\n",
    "    while len(data) < n:\n",
    "        trajectory = collect_trajectory(policy, feature_dim)\n",
    "        i = 0\n",
    "        while i < len(trajectory)-3:\n",
    "            state = trajectory[i]\n",
    "            action = policy(state)\n",
    "            phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "            reward = trajectory[i+2]\n",
    "            next_state = trajectory[i+3]\n",
    "            next_action = policy(next_state)\n",
    "            phi_sa_prime = rbf_random_fourier_features(next_state, next_action, feature_dim)\n",
    "            \n",
    "            data.append((phi_sa, reward, phi_sa_prime))\n",
    "            i += 3\n",
    "            if len(data) >= n:\n",
    "                break\n",
    "\n",
    "    return data[:n]  # Return exactly n samples as a single array\n",
    "\n",
    "def Q(state, action, theta,feature_dim=feature_dim):\n",
    "    phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    return np.dot(theta, phi_sa)\n",
    "\n",
    "def policy_eval_LSTD(theta_init,data, feature_dim=feature_dim, alpha=0.01):\n",
    "    '''Use TD(0) which converges to the solution of LSTD'''\n",
    "    theta_lstd = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        Q_sa = np.dot(theta_lstd, phi_sa)\n",
    "        Q_sa_prime = np.dot(theta_lstd, phi_sa_prime)\n",
    "        td_error = reward + gamma * Q_sa_prime - Q_sa\n",
    "        theta_lstd += alpha * td_error * phi_sa\n",
    "    \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_lstd, phi_sa)\n",
    "    \n",
    "    return theta_lstd\n",
    "\n",
    "def policy_eval_BRM(theta_init, data,  feature_dim=feature_dim, learning_rate=0.1):\n",
    "    theta_BRM = np.copy(theta_init)\n",
    "    for phi_sa, reward, phi_sa_prime in data:\n",
    "        x_sa = phi_sa - gamma * phi_sa_prime\n",
    "        gradient = -2 * (reward - np.dot(x_sa, theta_BRM)) * x_sa\n",
    "        theta_BRM -= learning_rate * gradient\n",
    "        \n",
    "    # def Q(state, action):\n",
    "    #     phi_sa = rbf_random_fourier_features(state, action, feature_dim)\n",
    "    #     return np.dot(theta_BRM, phi_sa)\n",
    "    \n",
    "    return theta_BRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_data = collect_data(n_samples, policy_PPO, feature_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04995869 0.00257682 0.02502597 0.00128588] 0\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "action = env.action_space.sample()\n",
    "print(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = np.zeros(feature_dim)\n",
    "Q_lstd = policy_eval_LSTD(theta_init, offline_data)\n",
    "Q_BRM = policy_eval_BRM(theta_init, offline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grids = 2\n",
    "def index_to_state_action(i, n_grid_points=num_grids):\n",
    "    \"\"\"\n",
    "    Maps index i (0 to 2*n_grid_points^4-1) to a state-action pair\n",
    "    \n",
    "    Returns:\n",
    "    - state: np.array of shape (4,)\n",
    "    - action: int (0 or 1)\n",
    "    \"\"\"\n",
    "    # State bounds\n",
    "    state_bounds = [\n",
    "        [-4.8, 4.8],     # cart position\n",
    "        [-10.0, 10.0],   # cart velocity\n",
    "        [-0.418, 0.418], # pole angle\n",
    "        [-10.0, 10.0]    # pole angular velocity\n",
    "    ]\n",
    "    \n",
    "    # Total states per dimension\n",
    "    n_states = n_grid_points**4\n",
    "    \n",
    "    # Determine action (0 for first half indices, 1 for second half)\n",
    "    action = 1 if i>= n_states else 0\n",
    "    \n",
    "    # Get state index (map back to state space)\n",
    "    state_idx = i % n_states\n",
    "    \n",
    "    # Convert to grid coordinates\n",
    "    idx_4 = state_idx % n_grid_points\n",
    "    idx_3 = (state_idx // n_grid_points) % n_grid_points\n",
    "    idx_2 = (state_idx // (n_grid_points**2)) % n_grid_points\n",
    "    idx_1 = state_idx // (n_grid_points**3)\n",
    "    \n",
    "    # Convert grid coordinates to actual state values\n",
    "    state = np.array([\n",
    "        np.linspace(state_bounds[0][0], state_bounds[0][1], n_grid_points)[idx_1],\n",
    "        np.linspace(state_bounds[1][0], state_bounds[1][1], n_grid_points)[idx_2],\n",
    "        np.linspace(state_bounds[2][0], state_bounds[2][1], n_grid_points)[idx_3],\n",
    "        np.linspace(state_bounds[3][0], state_bounds[3][1], n_grid_points)[idx_4]\n",
    "    ])\n",
    "    \n",
    "    return state, action\n",
    "\n",
    "# Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_evaluation_pairs(policy, num_grids = num_grids, n_episodes=100, max_steps=500):\n",
    "    \"\"\"\n",
    "    Estimates Q values using given policy for trajectories\n",
    "    \n",
    "    Args:\n",
    "        policy: Function that takes state and returns action\n",
    "        num_grids: Number of grid points per dimension\n",
    "        n_episodes: Number of episodes per state-action pair\n",
    "        max_steps: Maximum steps per episode\n",
    "        \n",
    "    Returns:\n",
    "        Q_vector: Estimated Q-values for each state-action pair\n",
    "    \"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "    total_pairs = 2 * num_grids**4\n",
    "    Q_vector = np.zeros(total_pairs)\n",
    "    \n",
    "    for i in tqdm(range(total_pairs)):\n",
    "        state, action = index_to_state_action(i, num_grids)\n",
    "        returns = []\n",
    "        \n",
    "        for _ in range(n_episodes):\n",
    "            env.reset()\n",
    "            env.state = state\n",
    "            \n",
    "            # Take specified initial action\n",
    "            next_state, reward, term, trunc, _ = env.step(action)\n",
    "            total_return = reward\n",
    "            \n",
    "            # Continue with policy-chosen actions\n",
    "            discount = gamma\n",
    "            curr_state = next_state\n",
    "            steps = 0\n",
    "            \n",
    "            while not (term or trunc) and steps < max_steps:\n",
    "                # Use policy to select action\n",
    "                curr_action = policy(curr_state)\n",
    "                curr_state, r, term, trunc, _ = env.step(curr_action)\n",
    "                total_return += discount * r\n",
    "                discount *= gamma\n",
    "                steps += 1\n",
    "                \n",
    "                if discount < 1e-10:\n",
    "                    break\n",
    "                    \n",
    "            returns.append(total_return)\n",
    "            \n",
    "        Q_vector[i] = np.mean(returns)\n",
    "    \n",
    "    env.close()\n",
    "    return Q_vector\n",
    "def loss_policy_evaluation(theta, Q_real, num_grids = num_grids):\n",
    "    loss = 0 \n",
    "    total_pairs = 2 * num_grids**4\n",
    "    for i in range(total_pairs):\n",
    "        state, action = index_to_state_action(i, num_grids)\n",
    "        Q_est_i = Q(state, action, theta)\n",
    "        loss += (Q_est_i- Q_real[i])**2\n",
    "    loss /= total_pairs\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q_real = grid_evaluation_pairs(policy_PPO)\n",
    "# np.save(f\"Q_function.npy\", Q_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_real = np.load(f\"Q_function.npy\")\n",
    "iter = int( n_samples / 50 )\n",
    "loss_LSTD = [0] * int(n_samples/ iter)\n",
    "loss_BRM = [0] * int(n_samples/ iter)\n",
    "total_pairs = 2 * num_grids**4\n",
    "\n",
    "l2_norm_diff_BRM_list = []\n",
    "l2_norm_diff_LSTD_list = []\n",
    "\n",
    "\n",
    "theta_lstd = np.zeros(feature_dim)\n",
    "theta_BRM = np.zeros(feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "980\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for m in range(iter, n_samples + 1, iter):\n",
    "    \n",
    "    offline_data = collect_data(iter, policy_PPO, feature_dim)\n",
    "    theta_lstd = policy_eval_LSTD(theta_lstd, offline_data)\n",
    "    theta_BRM = policy_eval_BRM(theta_BRM, offline_data)\n",
    "    loss_LSTD_m = loss_policy_evaluation(theta_lstd, Q_real)\n",
    "    loss_BRM_m = loss_policy_evaluation(theta_BRM, Q_real)\n",
    "\n",
    "    l2_norm_diff_LSTD_list.append(loss_LSTD_m)\n",
    "    l2_norm_diff_BRM_list.append(loss_BRM_m)\n",
    "    print(m)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm_diff_LSTD_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (50,) and (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_norm_diff_BRM_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBRM Loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28miter\u001b[39m, n_samples \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28miter\u001b[39m), l2_norm_diff_LSTD_list,linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-.\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTD Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Data Points\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xuanf\\miniconda3\\envs\\policy-eval\\Lib\\site-packages\\matplotlib\\pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xuanf\\miniconda3\\envs\\policy-eval\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\xuanf\\miniconda3\\envs\\policy-eval\\Lib\\site-packages\\matplotlib\\axes\\_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xuanf\\miniconda3\\envs\\policy-eval\\Lib\\site-packages\\matplotlib\\axes\\_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (50,) and (100,)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(range(iter, n_samples + 1, iter), l2_norm_diff_BRM_list, label='BRM Loss', color='red')\n",
    "\n",
    "plt.plot(range(iter, n_samples + 1, iter), l2_norm_diff_LSTD_list,linestyle='-.', label='LSTD Loss', color='blue')\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.ylabel('L2 Norm Difference')\n",
    "plt.yscale('log')\n",
    "plt.title('Loss Curves for BRM and LSTD')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('plot_image.pdf', bbox_inches='tight')          # Save as PDF\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
